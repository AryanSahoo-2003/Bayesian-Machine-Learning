{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2b3c38",
   "metadata": {},
   "source": [
    "# Bayesian Machine Learning for Health Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94a280",
   "metadata": {},
   "source": [
    "### Task 2 : -  Briefly explain and implement from scratch the following functions: i) cross-entropy; ii) entropy; iii) mutual information; iv) conditional entropy;   v) KL divergence. Take appropriate example toy data/distributions and explain the insights from calculating these quantities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85d09e",
   "metadata": {},
   "source": [
    "###### reference: https://tungmphung.com/information-theory-concepts-entropy-mutual-information-kl-divergence-and-more/\n",
    "\n",
    "\n",
    "#### a) Entropy\n",
    "This measures the degree of uncertainty in the events.The fundamental tenet of this expression is that we cannot determine which event is more likely to occur in reality if both events have comparable chances of occurring (entropy is high).\n",
    "But let's imagine that the likelihood of some events occurring is significantly larger than that of the others. In such case, we may argue that, in reality, the event that is most likely to occur will be one of the events from the set with the higher probability (entropy is less). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b470d8",
   "metadata": {},
   "source": [
    "![alternatvie text](https://tungmphung.com/wp-content/ql-cache/quicklatex.com-6e5fe46a0e1159891d09684d0fe9d8b7_l3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    e = 0\n",
    "    for i in range(len(p)):\n",
    "        if p[i] > 0:\n",
    "            e -= p[i] * math.log(p[i])\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11ce55",
   "metadata": {},
   "source": [
    "Example:\n",
    "Suppose we have a biased coin that has a probability of 0.8 of landing heads and 0.2 of landing tails. The entropy of the distribution is:\n",
    "\n",
    "H(p) = - (0.8 * log(0.8) + 0.2 * log(0.2)) = 0.502\n",
    "\n",
    "Now take it to be same (0.5) for both of them. Now H(p) = - (0.5 * log(0.5) + 0.5 * log(0.5)) = 0.3010\n",
    "\n",
    "Insights: A higher entropy value indicates more randomness or uncertainty in the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050731fa",
   "metadata": {},
   "source": [
    "#### b) Cross-entropy:\n",
    "Cross-entropy is a measure of the difference between two probability distributions, typically the predicted probability distribution and the true probability distribution. It is commonly used in machine learning for evaluating classification models. The formula for cross-entropy between two probability distributions p and q is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce5ffb",
   "metadata": {},
   "source": [
    "![cross-entropy](https://tungmphung.com/wp-content/ql-cache/quicklatex.com-e5165ec6015917d7491e82de9b96959f_l3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(a,b):\n",
    "    hE=0\n",
    "    assert len(a) == len(b)\n",
    "    for i in range(0,len(a)):\n",
    "        if p[i] > 0:\n",
    "            h-=a[i]*math.log(b[i])\n",
    "    return hE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118a0fe",
   "metadata": {},
   "source": [
    "Example:\n",
    "Suppose we have a binary classification problem with two classes A and B. We have a model that predicts the probability of each class given an input. We also have a true distribution over the classes. Let's say the true distribution is p = [0.6, 0.4] and the predicted distribution by the model is q = [0.8, 0.2]. Then the cross-entropy between p and q can be calculated as follows:\n",
    "\n",
    "H(p,q) = - (0.6 * log(0.8) + 0.4 * log(0.2)) = 0.289\n",
    "\n",
    "Insights: A lower value of cross-entropy indicates a better match between the true distribution and the predicted distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fc1cd",
   "metadata": {},
   "source": [
    "#### c) Mutual Information:\n",
    "\n",
    "Mutual information is a measure of the amount of information that one random variable contains about another random variable.This is related to the Conditional Entropy, as Mutual Information is the amount of reduced uncertainty about A if we already know B. Given two random variables X and Y, the mutual information between X and Y is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8626e",
   "metadata": {},
   "source": [
    "![mutual-information](https://tungmphung.com/wp-content/ql-cache/quicklatex.com-fd3b6af9f9c9822135108e3906181eaf_l3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(p_joint, p_X, p_Y):\n",
    "    p_X_Y = p_joint / p_Y[:, np.newaxis]\n",
    "    return entropy(p_X) - entropy(p_X_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e31aae",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Suppose we have two random variables X and Y, and their joint probability distribution is:\n",
    "\n",
    "p_joint = [[0.2, 0.3],\n",
    "           [0.1, 0.4]]\n",
    "\n",
    "The marginal probability distributions of X and Y are:\n",
    "\n",
    "p_X = [0.3, 0.6]\n",
    "p_Y = [0.3, 0.7]\n",
    "\n",
    "Then, the mutual information between X and Y is:\n",
    "\n",
    "I(p_joint, p_x, p_y) = H(X) - H(X|Y) = (-[(0.3 * log(0.3)) + (0.6 * log(0.6))]) - (-[(0.2log(0.2/0.3)) + (0.3log(0.3/0.7)) + (0.1log(0.1/0.3)) + (0.4log(0.4/0.7))]) = 0.076\n",
    "\n",
    "Insight:\n",
    "\n",
    "The mutual information measures the amount of information that one random variable contains about another random variable. In this example, we can see that X and Y are not independent, as their mutual information is greater than zero. We can also see that X and Y are not perfectly correlated, as their mutual information is not equal to the entropy of either X or Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc591e",
   "metadata": {},
   "source": [
    "#### d) Conditional Entropy:\n",
    "\n",
    "Conditional entropy is a measure of the amount of uncertainty or randomness in a random variable X given another random variable Y. Given two random variables X and Y, the conditional entropy of X given Y is defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770cd49",
   "metadata": {},
   "source": [
    "![cond-entropy](https://tungmphung.com/wp-content/ql-cache/quicklatex.com-49c58e49cea026276221bb464762ca43_l3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29958b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_Entropy(p_joint, p_y):\n",
    "    p_x_given_y = p_joint / p_y[:, np.newaxis]\n",
    "    return np.sum(-p_y * np.sum(p_x_given_y * np.log(p_x_given_y), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81abdf",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Suppose we have two random variables X and Y, and their joint probability distribution is:\n",
    "\n",
    "p_joint = [[0.2, 0.3],\n",
    "[0.1, 0.4]]\n",
    "\n",
    "The marginal probability distribution of Y is:\n",
    "\n",
    "p_y = [0.3, 0.7]\n",
    "\n",
    "Then, the conditional entropy of X given Y is:\n",
    "\n",
    "H(X|Y) = -[(0.3 * [(0.2log(0.2/0.3)) + (0.3log(0.3/0.7))]) + (0.7 * [(0.1log(0.1/0.3)) + (0.4log(0.4/0.7))])] = 0.549\n",
    "\n",
    "Insight:\n",
    "The conditional entropy measures the amount of uncertainty or randomness in a random variable X given another random variable Y. In this example, we can see that the conditional entropy of X given Y is smaller than the entropy of X, which indicates that knowing Y reduces the uncertainty or randomness in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa9c69",
   "metadata": {},
   "source": [
    "#### e) KL Divergence:\n",
    "\n",
    "KL divergence is a measure of the difference between two probability distributions.The amount of redundant information we would typically have if we encoded A using the optimal encoding method of B is measured by the Kullback-Leibler Divergence of A from B, often known as the KL Divergence. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea86c2",
   "metadata": {},
   "source": [
    "![kl-divergence](https://tungmphung.com/wp-content/ql-cache/quicklatex.com-1b9ee98ad72b18fc48359a05edec4e0e_l3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8348d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "      return crossEntropy(p,q) - entropy(p) #already created the functions above in a) and b) parts of task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a3571",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Suppose we have two probability distributions, p and q:\n",
    "\n",
    "p = [0.3, 0.7]\n",
    "q = [0.4, 0.6]\n",
    "\n",
    "Then, the KL divergence of p and q is:\n",
    "    Dkl(p || q) = (0.3log(0.3/0.4)) + (0.7log(0.7/0.6)) = 0.046\n",
    "\n",
    "Insight:\n",
    "\n",
    "The KL divergence measures the difference between two probability distributions. In this example, we can see that p and q are different, as their KL divergence is greater than zero. We can also see that the KL divergence is not symmetric, as Dkl(p || q) is different from Dkl(q || p).\n",
    "\n",
    "###### I think that the KL divergence is not a true distance metric, as it does not satisfy the triangle inequality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64900f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
